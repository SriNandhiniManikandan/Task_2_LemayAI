# HuggingFace Inference API â€“ "askSri"

This project implements a FastAPI-based container-ready backend service that allows you to send inference requests to a pre-trained NLP model hosted on HuggingFace. It supports parallel requests and is ready to scale in production environments with tools like `uvicorn`, `gunicorn`, and `NGINX`.

---

## ğŸš€ Purpose

This API exposes a `/askSri/generate` endpoint to process text prompts and return generated responses using a transformer model. It demonstrates how to:
- Serve HuggingFace models in a clean API format
- Support multiple requests concurrently
- Containerize and deploy inference services efficiently

---

## ğŸ¤– Model Used

**Model**: `google/flan-t5-base`

### âœ… Why this model?
- It's a **lightweight**, instruction-tuned variant of the T5 family.
- Handles summarization, translation, Q&A, and general text generation tasks.
- Fast to load and works well on CPU-only machines.
- Great choice for demonstration or proof-of-concept APIs without needing a GPU.

---

## ğŸ“‚ Project Structure

LLM project/
â”œâ”€â”€ app/
â”‚ â”œâ”€â”€ api/
â”‚ â”‚ â”œâ”€â”€ init.py # Exposes the router
â”‚ â”‚ â””â”€â”€ endpoints.py # FastAPI endpoint for inference
â”‚ â”œâ”€â”€ init.py # Loads the model once at startup
â”‚ â”œâ”€â”€ main.py # Initializes the FastAPI app
â”‚ â””â”€â”€ model_loader.py # Handles model loading and response generation
â”œâ”€â”€ logs/
â”‚ â”œâ”€â”€ app.log # Runtime logs
â”‚ â””â”€â”€ logging_config.py # Logging configuration
â”œâ”€â”€ requirements.txt # All required dependencies
â”œâ”€â”€ README.md # Project overview (this file)


---

## ğŸ› ï¸ How to Run

### ğŸ”¹ 1. Create and activate a virtual environment:
```bash
python -m venv venv
.\venv\Scripts\activate   # Windows
# or
source venv/bin/activate  # macOS/Linux
